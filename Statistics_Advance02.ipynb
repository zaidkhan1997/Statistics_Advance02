{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bbac1c-3fb4-4e29-b905-aba3dd9752ce",
   "metadata": {},
   "source": [
    "# Statistics_Advance02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a1e2af-633b-40da-8b45-96cafef91e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.1:Explain the properties of the F-distribution. \n",
    "\n",
    "\n",
    "#Answer: The F-distribution is a continuous probability distribution that arises in statistical analysis, especially in the context of comparing variances or testing hypotheses about population variances. Here are some key properties of the F-distribution:\n",
    "\n",
    "# Shape: The F-distribution is skewed to the right, meaning it has a longer tail on the right side. The shape of the distribution depends on two parameters, often referred to as degrees of freedom: one for the numerator (df₁) and one for the denominator (df₂). As the degrees of freedom increase, the distribution becomes less skewed and approaches a normal distribution.\n",
    "\n",
    "# Non-negative values: The values of an F-distributed random variable are always non-negative (greater than or equal to 0). This is because it represents the ratio of two variances, which cannot be negative.\n",
    "\n",
    "# Asymmetry: The F-distribution is not symmetric. It is positively skewed, particularly when the degrees of freedom in both the numerator and denominator are small.\n",
    "\n",
    "# Mean: The mean of the F-distribution is greater than or equal to 1, but it depends on the degrees of freedom of both the numerator and denominator. When both degrees of freedom are large, the mean approaches 1.\n",
    "\n",
    "# Variance: The variance of the F-distribution also depends on the degrees of freedom of the numerator and denominator. It can be large, particularly for small degrees of freedom.\n",
    "\n",
    "# Use in hypothesis testing: The F-distribution is primarily used in the context of analysis of variance (ANOVA), regression analysis, and tests for the equality of variances. It is used to assess whether the variance between groups is significantly different from within groups.\n",
    "\n",
    "# Tail behavior: The F-distribution has heavy tails, meaning that extreme values are more likely compared to a normal distribution. The tail behavior also depends on the degrees of freedom, with the tail becoming less heavy as the degrees of freedom increase.\n",
    "\n",
    "# Relation to Chi-square distribution: The F-distribution is related to the chi-square distribution. Specifically, it is the ratio of two scaled chi-square distributions, each divided by its respective degrees of freedom.\n",
    "\n",
    "# In summary, the F-distribution is a key tool in statistics, especially for comparing variances and performing tests like ANOVA, where it helps determine whether the variances across different groups are significantly different from each other. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ca3e1-baf5-4c64-96eb-1f2c374b1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.2:In which types of statistical tests is the F-distribution used, and why is it appropriate \n",
    "#for these tests?\n",
    "\n",
    "\n",
    "#Answer: The F-distribution is used in several important statistical tests, particularly those that involve comparing variances or testing relationships between multiple groups. Here are the main types of tests where the F-distribution is used and why it's appropriate for these tests:\n",
    "\n",
    "# Analysis of Variance (ANOVA):\n",
    "\n",
    "#Purpose: ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
    "#Why F-distribution: ANOVA tests the null hypothesis that all group means are equal. The F-statistic in ANOVA is the ratio of the variance between groups to the variance within groups. Since variances are always positive and the ratio of variances follows an F-distribution, it is used to determine whether the observed differences in means are larger than what could be expected by chance.\n",
    "\n",
    "# Regression Analysis (F-test in Regression):\n",
    "#Purpose: In multiple regression, the F-test is used to assess whether the model as a whole is a good fit for the data. It tests if at least one predictor variable has a significant linear relationship with the response variable.\n",
    "#Why F-distribution: The F-statistic in regression compares the fit of the model (explained variance) to the unexplained variance (residual variance). The ratio of these variances follows an F-distribution, allowing us to test if the model provides a significantly better fit than a model with no predictors.\n",
    "\n",
    "# Testing Equality of Variances (F-test for Variances):\n",
    "#Purpose: This test compares the variances of two populations to see if they are equal.\n",
    "#Why F-distribution: The F-statistic is the ratio of two sample variances (from each group). Because the ratio of two independent chi-square distributions (representing the variances) follows an F-distribution, it is appropriate for testing whether two population variances are equal.\n",
    "\n",
    "# Multivariate Analysis of Variance (MANOVA):\n",
    "#Purpose: MANOVA extends ANOVA by examining multiple dependent variables simultaneously. It tests for differences in the means of multiple groups across several outcomes.\n",
    "#Why F-distribution: Like ANOVA, MANOVA involves the ratio of between-group variance to within-group variance. However, it handles multiple dependent variables and uses an F-statistic to test hypotheses about the group differences in the multivariate context.\n",
    "\n",
    "# Analysis of Covariance (ANCOVA):\n",
    "#Purpose: ANCOVA combines ANOVA and regression. It tests whether population means of a dependent variable (DV) are equal across levels of a categorical independent variable (IV), while controlling for the effects of other continuous variables (covariates).\n",
    "#Why F-distribution: ANCOVA uses an F-statistic to test the significance of group differences while accounting for covariates. The model compares the variation explained by the categorical factor and covariates to the unexplained variation, using the F-distribution to assess the results.\n",
    "#Why the F-distribution is appropriate for these tests:\n",
    "# The F-distribution is used in these tests because it is based on the ratio of two independent estimates of variance, making it ideal for comparing how much variability is explained by different factors (e.g., between groups vs. within groups) or for comparing model fits in regression.\n",
    "# In all of these cases, the F-distribution helps determine whether the observed differences in variance (or model fit) are statistically significant, accounting for the sample size and variability within the data.\n",
    "# Thus, the F-distribution is central to hypothesis testing in cases where variances are involved, particularly when comparing multiple groups or testing models with more than one predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e641a-424a-4e06-b478-2bed5ccadeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.3: What are the key assumptions required for conducting an F-test to compare the variances of two\n",
    "#populations?\n",
    "\n",
    "#Answer: When conducting an F-test to compare the variances of two populations, there are several key assumptions that need to be met for the results to be valid:\n",
    "\n",
    "# Independence of Samples:\n",
    "#The two samples being compared must be independent of each other. This means that the data points in one sample should not influence the data points in the other sample. Each observation must be randomly selected from its respective population.\n",
    "\n",
    "# Normality of the Populations:\n",
    "#The data in both populations being compared should follow (or approximately follow) a normal distribution. This is particularly important because the F-test assumes that the populations from which the samples are drawn are normally distributed. If this assumption is violated, the results of the test may not be reliable, especially for small sample sizes.\n",
    "\n",
    "# Scale of Measurement:\n",
    "#The data should be continuous (interval or ratio scale), as variances are meaningful only for numerical data with a consistent scale.\n",
    "\n",
    "# Random Sampling:\n",
    "#Each sample should be a random sample from its respective population. This helps ensure that the samples are representative of the populations, reducing bias and enhancing the validity of the test.\n",
    "\n",
    "# Homogeneity of Variances:\n",
    "#While the F-test itself is used to compare variances, it assumes that the variances of the two populations are not identical, but it requires that they are homogeneous (i.e., the population variances are of similar magnitude). Significant deviations from this assumption can make the F-test unreliable.\n",
    "#If these assumptions are not met, alternative methods (e.g., non-parametric tests or transforming the data) may be used to compare the variances or account for violations of assumptions. For instance, if the normality assumption is violated, one might consider using a non-parametric test like the Levene's test or the Brown-Forsythe test, which are less sensitive to deviations from normality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652b372-440f-4bbb-aa7b-6e08d8c72a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.4:What is the purpose of ANOVA, and how does it differ from a t-test? \n",
    "\n",
    "#Answer: The purpose of ANOVA (Analysis of Variance) is to compare the means of three or more groups to determine if there is a statistically significant difference among them. ANOVA helps assess whether the variation in the dependent variable is due to differences between the groups or due to random chance. It allows researchers to test hypotheses involving multiple groups simultaneously, rather than performing multiple pairwise comparisons.\n",
    "\n",
    "# Key Purposes of ANOVA:\n",
    "\n",
    "#Test Differences Among Multiple Groups: It helps identify whether any significant differences exist between the means of three or more groups in an experiment or study.\n",
    "\n",
    "#Control Type I Error: Instead of conducting multiple t-tests between pairs of groups (which increases the risk of committing a Type I error), ANOVA provides a single test to evaluate all group differences at once.\n",
    "\n",
    "#Analyze Variance: It partitions the total variance observed in the data into components attributable to different sources (between-group variance and within-group variance), helping researchers understand how much of the variability is due to the treatment or group differences.\n",
    "\n",
    "# Difference Between ANOVA and a t-test:\n",
    "# Number of Groups:\n",
    "#ANOVA: Compares means across three or more groups simultaneously. It is ideal when the research involves more than two groups.\n",
    "#t-test: Compares the means between two groups. It is used when comparing just two groups at a time (e.g., treatment vs. control).\n",
    "\n",
    "# Multiple Comparisons:\n",
    "# ANOVA: It tests the overall null hypothesis that all group means are equal. If the result is significant, post-hoc tests (like Tukey's or Bonferroni) can be used to identify which specific groups differ from each other.\n",
    "# t-test: It evaluates the difference between two specific group means. When multiple comparisons are needed (e.g., comparing multiple pairs of groups), using t-tests increases the risk of Type I errors unless adjustments are made (like the Bonferroni correction).\n",
    "\n",
    "# Type of Question:\n",
    "#ANOVA: Used when the research question involves comparing more than two groups and understanding the overall variance caused by different group factors.\n",
    "#t-test: Typically used for testing simple hypotheses about the difference between two means, such as comparing the effectiveness of two treatments or the mean scores between two groups.\n",
    "\n",
    "#Statistical Framework:\n",
    "#ANOVA: Focuses on partitioning the total variance in the data into between-group and within-group variances. The F-statistic is used to determine if the between-group variance is significantly larger than the within-group variance.\n",
    "#t-test: Focuses on the difference between two sample means relative to the variability of those samples, with the test statistic being a t-value.\n",
    "\n",
    "# In summary, ANOVA is the preferred method when comparing the means of three or more groups, as it allows a more efficient and controlled approach to analyzing group differences. A t-test is used when comparing only two groups and is limited to simpler situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cdf2d0-7d4c-433b-8fc7-6a704823188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.5:Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
    "#than two groups.\n",
    "\n",
    "\n",
    "#Answer: You would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups for several key reasons:\n",
    "\n",
    "# 1. Control Type I Error Rate:\n",
    "#Issue with Multiple t-tests: When you perform multiple t-tests between pairs of groups (e.g., comparing group 1 vs. group 2, group 1 vs. group 3, and so on), each test carries a risk of making a Type I error (incorrectly rejecting the null hypothesis when it’s actually true). As the number of tests increases, the overall chance of making at least one Type I error increases. This is known as the inflation of the family-wise error rate.\n",
    "#Why ANOVA: A one-way ANOVA allows you to test for differences among all groups simultaneously with a single test, which controls the Type I error rate. It maintains the overall error rate and avoids the increased risk of false positives that comes with multiple t-tests.\n",
    "\n",
    "# 2. Efficiency:\n",
    "#Issue with Multiple t-tests: Conducting a large number of t-tests can be time-consuming and inefficient, especially when comparing more than two groups. It also makes interpretation more complex, as you would need to adjust for the fact that you are doing multiple comparisons.\n",
    "#Why ANOVA: One-way ANOVA condenses all group comparisons into one statistical test. It tests whether there is any overall difference between the means of the groups, saving time and reducing complexity.\n",
    "\n",
    "# 3. Overall Comparison:\n",
    "#Issue with Multiple t-tests: When using t-tests, you are only comparing pairs of groups at a time. This means you may fail to detect significant differences between some groups, or you may identify differences without considering the broader context of all groups together.\n",
    "#Why ANOVA: A one-way ANOVA examines the variance between all groups collectively. If the result is significant, it indicates that at least one group is different from the others, which gives a more comprehensive understanding of the data. It’s useful when you are interested in whether there is a significant difference anywhere among the groups, not just in specific pairwise comparisons.\n",
    "\n",
    "# 4. Post-Hoc Analysis:\n",
    "#Issue with Multiple t-tests: If you used multiple t-tests, you would need to adjust for multiple comparisons using methods like the Bonferroni correction. This adjustment can be overly conservative and reduce statistical power, making it harder to detect real differences.\n",
    "#Why ANOVA: If the one-way ANOVA is significant, you can follow it up with post-hoc tests (e.g., Tukey’s HSD) to identify which specific groups are different. These post-hoc tests are designed to control for the increased error rate due to multiple comparisons, allowing you to determine which pairs of groups differ without the drawbacks of multiple t-tests.\n",
    "\n",
    "# 5. Interpretation of Results:\n",
    "#Issue with Multiple t-tests: After running multiple t-tests, interpreting the results can become confusing, especially when deciding how to handle cases where the p-values are borderline or where you have conflicting results between tests.\n",
    "#Why ANOVA: The result of a one-way ANOVA provides a clear, single p-value indicating whether there are any significant differences between the groups. If the result is significant, it suggests that the group means are not all equal, and further investigation can be done through post-hoc comparisons.\n",
    "# Summary:\n",
    "#In summary, one-way ANOVA is preferable over multiple t-tests when comparing more than two groups because it controls the overall Type I error rate, is more efficient, provides a more comprehensive view of group differences, and allows for more reliable post-hoc analysis. Multiple t-tests, on the other hand, increase the risk of false positives and require adjustments that can decrease statistical power. ANOVA is a more powerful and appropriate choice for handling multiple groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7402c-bcfe-40f0-b8b7-eb4a3afb3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.6:Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
    "#How does this partitioning contribute to the calculation of the F-statistic?\n",
    "\n",
    "#Answer: In ANOVA, variance is partitioned into two components: between-group variance and within-group variance. This partitioning is essential for determining whether the differences between the groups are statistically significant and for calculating the F-statistic. Here’s how it works:\n",
    "\n",
    "# 1. Between-Group Variance (Variation Between Groups):\n",
    "#What it represents: Between-group variance measures how much the group means differ from the overall mean (the mean of all groups combined). It reflects the variation that can be attributed to the differences between the groups.\n",
    "#Why it's important: If the group means are very different from each other, we would expect a large between-group variance, suggesting that the group factor (the independent variable) has a strong effect on the dependent variable. A large between-group variance suggests that the group means are not all the same and that there may be a significant effect.\n",
    "\n",
    "# 2. Within-Group Variance (Variation Within Groups):\n",
    "#What it represents: Within-group variance measures the variation within each individual group. It reflects the natural variability or error within each group, representing individual differences or random fluctuations in the data.\n",
    "#Why it's important: A smaller within-group variance suggests that the data points within each group are more consistent or similar to each other, whereas a larger within-group variance indicates more spread out or dispersed data within the groups.\n",
    "\n",
    "# 3. Partitioning of Total Variance:\n",
    "#The total variance in the data (the total variability in the dependent variable) is the sum of between-group variance and within-group variance. This total variance comes from two sources:\n",
    "#Between-group variance: Variation due to differences in the group means.\n",
    "#Within-group variance: Variation within each group due to individual differences or random error.\n",
    "#The goal of ANOVA is to determine whether the between-group variance is significantly larger than the within-group variance. If the between-group variance is much larger, it suggests that the group factor (e.g., treatment, group condition) has a significant effect on the dependent variable.\n",
    "\n",
    "# 4. Calculation of the F-statistic:\n",
    "#The F-statistic is a ratio of the between-group variance to the within-group variance.\n",
    "#If the group means are very different (i.e., the between-group variance is large), and there is little variability within the groups (i.e., the within-group variance is small), the ratio will be large, indicating a significant difference between the groups.\n",
    "#If the group means are similar (i.e., the between-group variance is small), and the within-group variance is large, the ratio will be small, suggesting that any observed differences between the groups are due to random variability rather than a true effect.\n",
    "#Thus, the F-statistic is used to test the null hypothesis that all group means are equal. A large F-statistic indicates that the between-group variance is significantly greater than the within-group variance, suggesting that there are significant differences between the groups.\n",
    "\n",
    "# In Summary:\n",
    "#Between-group variance reflects the variation due to the differences between the groups.\n",
    "#Within-group variance reflects the variation within each group, often due to random error.\n",
    "#The F-statistic is calculated by comparing the between-group variance to the within-group variance. A large F-statistic suggests that the variation between groups is significantly greater than the variation within groups, indicating a significant difference in group means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7631e-ea44-4264-b6a2-58569b4bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question_No.7:Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
    "#differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
    "\n",
    "#Answer: The classical (frequentist) approach to ANOVA and the Bayesian approach to ANOVA differ in several key aspects, especially in how they handle uncertainty, parameter estimation, and hypothesis testing. Here's a comparison of the two:\n",
    "\n",
    "# 1. Handling Uncertainty:\n",
    "# Classical (Frequentist) Approach:\n",
    "#In the frequentist approach, uncertainty is primarily expressed in terms of sampling variability. For example, the standard error of an estimate represents the uncertainty about the population parameter based on the sample. The focus is on confidence intervals and p-values to make inferences about the population.\n",
    "#Uncertainty about parameters: In this approach, the parameters (such as the means and variances of the groups) are treated as fixed, unknown values, and the variability in the data is used to estimate these parameters.\n",
    "\n",
    "# Bayesian Approach:\n",
    "\n",
    "#The Bayesian approach, on the other hand, incorporates probability distributions to express uncertainty about parameters. Instead of treating parameters as fixed values, it views them as random variables that have their own distributions (called prior distributions). After observing the data, the prior distributions are updated to form a posterior distribution.\n",
    "#Uncertainty about parameters: In Bayesian ANOVA, uncertainty is expressed in terms of the posterior distributions of the parameters, which give the range of possible values for parameters after accounting for both the prior beliefs and the observed data.\n",
    "\n",
    "# 2. Parameter Estimation:\n",
    "# Classical (Frequentist) Approach:\n",
    "\n",
    "#Parameters (e.g., group means, variances) are estimated using point estimates such as the sample mean and sample variance. The frequentist approach doesn't directly provide a distribution for these parameters. Instead, it focuses on deriving confidence intervals that provide a range of likely values for the parameter based on the data.\n",
    "#Estimating group means and variances: Point estimates are obtained, and inference about group differences is based on the test statistics (e.g., F-statistic) derived from these estimates.\n",
    "\n",
    "# Bayesian Approach:\n",
    "\n",
    "#Parameters are estimated through the posterior distribution. This posterior distribution is obtained by combining prior beliefs (the prior distribution) with the likelihood of the observed data. The result is a distribution of parameter values, from which summaries such as the mean, median, or credible intervals can be extracted.\n",
    "#Estimating group means and variances: Bayesian methods estimate the full distribution of parameters (e.g., the means of the groups) rather than a single point estimate, providing a richer understanding of the uncertainty.\n",
    "\n",
    "# 3. Hypothesis Testing:\n",
    "# Classical (Frequentist) Approach:\n",
    "\n",
    "#The frequentist approach uses p-values to assess the evidence against the null hypothesis. A hypothesis test (e.g., F-test in ANOVA) is performed to determine whether the observed differences between group means are statistically significant. The null hypothesis is typically that all group means are equal, and if the p-value is below a predetermined threshold (e.g., 0.05), the null hypothesis is rejected.\n",
    "#Decision-making: The decision to reject or fail to reject the null hypothesis is based on the p-value, which measures the probability of obtaining results as extreme as those observed, assuming the null hypothesis is true.\n",
    "\n",
    "# Bayesian Approach:\n",
    "\n",
    "#In the Bayesian approach, hypothesis testing is typically done through the comparison of posterior probabilities. Instead of testing a null hypothesis, Bayesian methods calculate the probability of the data under different hypotheses. This is often done by calculating Bayes factors, which compare the likelihood of the data under different models or hypotheses.\n",
    "#Decision-making: Bayesian hypothesis testing provides a direct measure of the evidence in favor of one hypothesis over another, allowing researchers to make decisions in terms of the relative plausibility of different hypotheses, rather than simply rejecting or failing to reject a null hypothesis.\n",
    "\n",
    "#4. Interpretation of Results:\n",
    "# Classical (Frequentist) Approach:\n",
    "\n",
    "#Results are interpreted in terms of significance. A significant result (e.g., a small p-value) suggests that there is strong evidence to reject the null hypothesis. However, the frequentist approach does not provide direct probabilities about the parameters themselves.\n",
    "#Confidence intervals give a range of plausible values for the parameter, but they do not directly provide the probability that the parameter lies within the interval. Instead, they indicate how the parameter could vary across repeated sampling.\n",
    "\n",
    "# Bayesian Approach:\n",
    "\n",
    "#Results are interpreted in terms of probabilities. For example, Bayesian credible intervals give the probability that a parameter lies within a given range, conditioned on the data. This provides a more direct interpretation of uncertainty in terms of the parameters themselves.\n",
    "#The posterior probability of a hypothesis or parameter can be directly interpreted, offering a more intuitive understanding of uncertainty and evidence.\n",
    "\n",
    "# 5. Flexibility with Prior Information:\n",
    "# Classical (Frequentist) Approach:\n",
    "#The frequentist approach does not use prior information; it relies purely on the data from the current experiment to estimate parameters and test hypotheses.\n",
    "\n",
    "# Bayesian Approach:\n",
    "#The Bayesian approach incorporates prior knowledge or beliefs about the parameters through the prior distribution. This allows for more flexible modeling, especially in cases where data might be sparse or when there is prior expert knowledge about the system being studied. The prior is updated with the data to form the posterior, allowing for a more nuanced and dynamic understanding of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a971baae-58e9-4819-b27e-7efb4d1304c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.2484076433121016), np.float64(0.22595520576550499))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question_No.8:You have two sets of data representing the incomes of two different professions1\n",
    "#V Profession A2 [48, 52, 55, 6(, 62'\n",
    "#V Profession B2 [45, 5(, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "#incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "#Task2 Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "#Objective2 Gain experience in performing F-tests and interpreting the results in terms of variance comparison\n",
    "\n",
    "#Answer:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data for Profession A and Profession B\n",
    "profession_a = [48, 52, 55, 61, 62]\n",
    "profession_b = [45, 50, 55, 52, 47]\n",
    "\n",
    "# Calculate variances\n",
    "var_a = np.var(profession_a, ddof=1)\n",
    "var_b = np.var(profession_b, ddof=1)\n",
    "\n",
    "# Calculate F-statistic (larger variance / smaller variance)\n",
    "f_stat = var_a / var_b if var_a > var_b else var_b / var_a\n",
    "\n",
    "# Degrees of freedom for each group\n",
    "df_a = len(profession_a) - 1\n",
    "df_b = len(profession_b) - 1\n",
    "\n",
    "# Calculate p-value for the F-test\n",
    "p_value = stats.f.sf(f_stat, df_a, df_b)\n",
    "\n",
    "f_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c39a323-89ac-454a-8f10-704b0b285fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(67.87330316742101), np.float64(2.870664187937026e-07))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question_No.9: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "#average heights between three different regions with the following data1\n",
    "# Region A2 [16(, 162, 165, 158, 164'\n",
    "# Region B2 [172, 175, 17(, 168, 174'\n",
    "# Region C2 [18(, 182, 179, 185, 183'\n",
    "# Task2 Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "# Objective2 Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
    "\n",
    "#Answer:\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data for each region\n",
    "region_a = [160, 162, 165, 158, 164]\n",
    "region_b = [172, 175, 170, 168, 174]\n",
    "region_c = [180, 182, 179, 185, 183]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
    "\n",
    "# Output the F-statistic and p-value\n",
    "f_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d074b96-3a9c-49cf-aa3c-9587cec10bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
